{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a46dee8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MNIST - MNIST-M Domain Adaptation\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPool2D, BatchNormalization, Dropout\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import csc_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3f766c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f41676b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization (normal_df):\n",
    "    \n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    feature_names = list(normal_df)\n",
    "    \n",
    "    min_max_scaler = min_max_scaler.fit(normal_df.values)\n",
    "    X_raw_minmax = min_max_scaler.transform(normal_df.values)\n",
    "    normal_df = pd.DataFrame(X_raw_minmax, columns=feature_names)\n",
    "\n",
    "    return normal_df, min_max_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23206a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-27 16:08:22,553 W4633070102_W4662FM0400_data_loader\n",
      "2021-07-27 16:08:22,561 Note: NumExpr detected 36 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2021-07-27 16:08:22,562 NumExpr defaulting to 8 threads.\n",
      "2021-07-27 16:08:29,597 source shape:(100873, 396)\n",
      "2021-07-27 16:08:29,599 target shape:(43518, 396)\n"
     ]
    }
   ],
   "source": [
    "#CONSTANTS\n",
    "#MNIST_M_PATH = './Datasets/MNIST_M/mnistm.h5'\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "CHANNELS = 3\n",
    "EPOCH = 10\n",
    "\n",
    "source_df, target_df, tag_dict = csc_dataloader.loader('W4633070102', 'W4662FM0400')\n",
    "source_df = source_df.sort_index()\n",
    "target_df = target_df.sort_index()\n",
    "\n",
    "source_df = csc_dataloader.labeler(source_df, \n",
    "                                    tag_dict['source_training_from'], \n",
    "                                    tag_dict['source_training_to'],\n",
    "                                    tag_dict['source_end'])\n",
    "\n",
    "\n",
    "target_df = csc_dataloader.labeler(target_df, \n",
    "                                    tag_dict['target_training_from'], \n",
    "                                    tag_dict['target_training_to'],\n",
    "                                    tag_dict['target_end'])\n",
    "\n",
    "source_x = source_df.drop(columns=['label'])\n",
    "target_x = target_df.drop(columns=['label'])\n",
    "\n",
    "source_x, normalizer = normalization(source_x)\n",
    "target_x = normalizer.transform(target_x)\n",
    "\n",
    "#target_y = tf.one_hot(target_df['label'], depth=2)\n",
    "#source_y = tf.one_hot(source_df['label'], depth=2)\n",
    "\n",
    "\n",
    "source_train_x, source_test_x, source_train_y, source_test_y = train_test_split(source_x, source_df['label'], test_size=640, shuffle=False)\n",
    "target_train_x, target_test_x, target_train_y, target_test_y = train_test_split(target_x, target_df['label'], test_size=640, shuffle=False)\n",
    "\n",
    "source_train_y = tf.one_hot(source_train_y, depth=2)\n",
    "source_test_y = tf.one_hot(source_test_y, depth=2)\n",
    "target_train_y = tf.one_hot(target_train_y, depth=2)\n",
    "target_test_y = tf.one_hot(target_test_y, depth=2)\n",
    "\n",
    "\n",
    "source_dataset = tf.data.Dataset.from_tensor_slices((source_train_x, source_train_y)).batch(BATCH_SIZE * 2)                                \n",
    "da_dataset = tf.data.Dataset.from_tensor_slices((source_train_x, source_train_y, target_train_x, target_train_y )).batch(BATCH_SIZE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((source_test_x, source_test_y)).batch(BATCH_SIZE * 2) #Test Dataset over Target Domain\n",
    "test_dataset2 = tf.data.Dataset.from_tensor_slices((target_test_x, target_test_y)).batch(BATCH_SIZE * 2) #Test Dataset over Target (used for training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7d63bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Reversal Layer\n",
    "@tf.custom_gradient\n",
    "def gradient_reverse(x, lamda=1.0):\n",
    "    y = tf.identity(x)\n",
    "    \n",
    "    def grad(dy):\n",
    "        return lamda * -dy, None\n",
    "    \n",
    "    return y, grad\n",
    "\n",
    "\n",
    "class GradientReversalLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def call(self, x, lamda=1.0):\n",
    "        return gradient_reverse(x, lamda)\n",
    "\n",
    "\n",
    "class DANN(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.feature_extractor_layer0 = Dense(400, activation='relu')\n",
    "        self.feature_extractor_layer1 = Dense(100, activation='relu')\n",
    "        self.feature_extractor_layer2 = Dense(20, activation='relu')\n",
    "        \n",
    "        #Label Predictor\n",
    "        self.label_predictor_layer0 = Dense(20, activation='relu')\n",
    "        self.label_predictor_layer1 = Dense(10, activation='relu')\n",
    "        self.label_predictor_layer2 = Dense(2, activation=None)\n",
    "        \n",
    "        #Domain Predictor\n",
    "        self.domain_predictor_layer0 = GradientReversalLayer()\n",
    "        self.domain_predictor_layer1 = Dense(20, activation='relu')\n",
    "        self.domain_predictor_layer2 = Dense(2, activation=None)\n",
    "        \n",
    "    def call(self, x, train=False, source_train=True, lamda=1.0):\n",
    "        #Feature Extractor\n",
    "        x = self.feature_extractor_layer0(x)\n",
    "        x = self.feature_extractor_layer1(x, training=train)\n",
    "        x = self.feature_extractor_layer2(x)\n",
    "\n",
    "        #print('DANN / feature shape:', x.shape)\n",
    "        #feature = tf.reshape(x, [-1, 64])\n",
    "        feature = x\n",
    "        \n",
    "        \n",
    "        #Label Predictor\n",
    "        if source_train is True:\n",
    "            feature_slice = feature\n",
    "        else:\n",
    "            feature_slice = tf.slice(feature, [0, 0], [feature.shape[0] // 2, -1])\n",
    "            \n",
    "        #feature_slice = feature\n",
    "        \n",
    "        lp_x = self.label_predictor_layer0(feature_slice)\n",
    "        lp_x = self.label_predictor_layer1(lp_x)\n",
    "        l_logits = self.label_predictor_layer2(lp_x)\n",
    "        #print('DANN / l_logits:', l_logits)\n",
    "        \n",
    "        #Domain Predictor\n",
    "        if source_train is True:\n",
    "            return l_logits\n",
    "        else:\n",
    "            dp_x = self.domain_predictor_layer0(feature, lamda)    #GradientReversalLayer\n",
    "            dp_x = self.domain_predictor_layer1(dp_x)\n",
    "            d_logits = self.domain_predictor_layer2(dp_x)\n",
    "            \n",
    "            #print('DANN / d_logits:', d_logits)\n",
    "            \n",
    "            return l_logits, d_logits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def loss_func(input_logits, target_labels):\n",
    "    #print('loss_func / input_logits', input_logits)\n",
    "    #print('loss_func / target_labels', target_labels)\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=input_logits, labels=target_labels))\n",
    "\n",
    "\n",
    "def get_loss(l_logits, labels, d_logits=None, domain=None):\n",
    "    #print('get_loss / l_logits', l_logits)\n",
    "    #print('get_loss / labels', labels)\n",
    "    \n",
    "    if d_logits is None:\n",
    "        return loss_func(l_logits, labels)\n",
    "    else:\n",
    "        return loss_func(l_logits, labels) + loss_func(d_logits, domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9423786",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DANN()\n",
    "model_optimizer = tf.optimizers.SGD()\n",
    "\n",
    "domain_labels = np.vstack([np.tile([1., 0.], [BATCH_SIZE, 1]),\n",
    "                           np.tile([0., 1.], [BATCH_SIZE, 1])])\n",
    "domain_labels = domain_labels.astype('float32')\n",
    "\n",
    "\n",
    "epoch_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "source_acc = []  # Source Domain Accuracy while Source-only Training\n",
    "da_acc = []      # Source Domain Accuracy while DA-training\n",
    "test_acc = []    # Testing Dataset (Target Domain) Accuracy \n",
    "test2_acc = []   # Target Domain (used for Training) Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e533fbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_source(s_images, s_labels, lamda=1.0):\n",
    "    images = s_images\n",
    "    labels = s_labels\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        output = model(images, train=True, source_train=True, lamda=lamda)\n",
    "        #print('output', output)\n",
    "        #print('labels', labels)\n",
    "        model_loss = get_loss(output, labels)\n",
    "        epoch_accuracy(output, labels)\n",
    "        \n",
    "    gradients_mdan = tape.gradient(model_loss, model.trainable_variables)\n",
    "    model_optimizer.apply_gradients(zip(gradients_mdan, model.trainable_variables))\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step_da(s_images, s_labels, t_images=None, t_labels=None, lamda=1.0):\n",
    "    images = tf.concat([s_images, t_images], 0)\n",
    "    labels = s_labels\n",
    "    \n",
    "    #print('train_step_da / s_labels', s_labels)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        output = model(images, train=True, source_train=False, lamda=lamda)\n",
    "\n",
    "        #print('train_step_da / output', output)\n",
    "        #print('train_step_da / labels', labels)\n",
    "        \n",
    "        model_loss = get_loss(output[0], labels, output[1], domain_labels)\n",
    "        epoch_accuracy(output[0], labels)\n",
    "        \n",
    "    gradients_mdan = tape.gradient(model_loss, model.trainable_variables)\n",
    "    model_optimizer.apply_gradients(zip(gradients_mdan, model.trainable_variables))\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def test_step(t_images, t_labels):\n",
    "    images = t_images\n",
    "    labels = t_labels\n",
    "    \n",
    "    output = model(images, train=False, source_train=True)\n",
    "    epoch_accuracy(output, labels)\n",
    "\n",
    "\n",
    "def train(train_mode, epochs=EPOCH):\n",
    "    \n",
    "    if train_mode == 'source':\n",
    "        dataset = source_dataset\n",
    "        train_func = train_step_source\n",
    "        acc_list = source_acc\n",
    "    elif train_mode == 'domain-adaptation':\n",
    "        dataset = da_dataset\n",
    "        train_func = train_step_da\n",
    "        acc_list = da_acc\n",
    "    else:\n",
    "        raise ValueError(\"Unknown training Mode\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        p = float(epoch) / epochs\n",
    "        lamda = 2 / (1 + np.exp(-100 * p, dtype=np.float32)) - 1\n",
    "        lamda = lamda.astype('float32')\n",
    "\n",
    "        for batch in dataset:\n",
    "            #print('train_func batch', batch.shape)\n",
    "            train_func(*batch, lamda=lamda)\n",
    "        \n",
    "        print(\"Training: Epoch {} :\\t Source Accuracy : {:.3%}\".format(epoch, epoch_accuracy.result()), end='  |  ')\n",
    "        acc_list.append(epoch_accuracy.result())\n",
    "        test()\n",
    "        epoch_accuracy.reset_states()\n",
    "\n",
    "\n",
    "def test():\n",
    "    epoch_accuracy.reset_states()\n",
    "    \n",
    "    #Testing Dataset (Target Domain)\n",
    "    for batch in test_dataset:\n",
    "        test_step(*batch)\n",
    "        \n",
    "    print(\"Testing Accuracy : {:.3%}\".format(epoch_accuracy.result()), end='  |  ')\n",
    "    test_acc.append(epoch_accuracy.result())\n",
    "    epoch_accuracy.reset_states()\n",
    "    \n",
    "    #Target Domain (used for Training)\n",
    "    for batch in test_dataset2:\n",
    "        test_step(*batch)\n",
    "    \n",
    "    print(\"Target Domain Accuracy : {:.3%}\".format(epoch_accuracy.result()))\n",
    "    test2_acc.append(epoch_accuracy.result())\n",
    "    epoch_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716c8d44",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch 0 :\t Source Accuracy : 91.143%  |  Testing Accuracy : 100.000%  |  Target Domain Accuracy : 100.000%\n",
      "Training: Epoch 1 :\t Source Accuracy : 86.542%  |  Testing Accuracy : 100.000%  |  Target Domain Accuracy : 100.000%\n",
      "Training: Epoch 2 :\t Source Accuracy : 86.393%  |  Testing Accuracy : 100.000%  |  Target Domain Accuracy : 100.000%\n",
      "Training: Epoch 3 :\t Source Accuracy : 86.393%  |  Testing Accuracy : 100.000%  |  Target Domain Accuracy : 100.000%\n",
      "Training: Epoch 4 :\t Source Accuracy : 86.393%  |  Testing Accuracy : 100.000%  |  Target Domain Accuracy : 100.000%\n",
      "Training: Epoch 5 :\t Source Accuracy : 86.393%  |  Testing Accuracy : 100.000%  |  Target Domain Accuracy : 100.000%\n",
      "Training: Epoch 6 :\t Source Accuracy : 86.393%  |  Testing Accuracy : 100.000%  |  Target Domain Accuracy : 100.000%\n",
      "Training: Epoch 7 :\t Source Accuracy : 86.393%  |  Testing Accuracy : 100.000%  |  Target Domain Accuracy : 100.000%\n",
      "Training: Epoch 8 :\t Source Accuracy : 86.393%  |  Testing Accuracy : 100.000%  |  Target Domain Accuracy : 100.000%\n",
      "Training: Epoch 9 :\t Source Accuracy : 86.393%  |  Testing Accuracy : 100.000%  |  Target Domain Accuracy : 100.000%\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "#train('source', 5)\n",
    "\n",
    "train('domain-adaptation', EPOCH)\n",
    "\n",
    "\n",
    "#Plot Results\n",
    "x_axis = [i for i in range(0, EPOCH)]\n",
    "\n",
    "plt.plot(x_axis, da_acc, label=\"source accuracy\")\n",
    "plt.plot(x_axis, test_acc, label=\"testing accuracy\")\n",
    "plt.plot(x_axis, test2_acc, label=\"target accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20c4572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d03424",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a116c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
