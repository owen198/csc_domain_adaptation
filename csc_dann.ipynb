{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "csc_dann.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPiBYdaOJDCjEcqLMTqJr3H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/owen198/csc_domain_adaptation/blob/main/csc_dann.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cllzfZXKefe3"
      },
      "source": [
        "import numpy as np\n",
        "from keras.layers import Input, Dense, Activation, BatchNormalization, PReLU, Dropout\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "import datetime\n",
        "from sklearn import preprocessing"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyhEdW3pjJ9G",
        "outputId": "bd546cba-cf45-49a0-f8cb-a7813a2a04fa"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqZoSyFTjKEk"
      },
      "source": [
        "tag1 = 'W4662FM0507'\n",
        "tag2 = 'W4662FM0606'\n",
        "path = './gdrive/My Drive/data/CSC/W4/'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tHMcFwSjKHO"
      },
      "source": [
        "tag1_pd = pd.concat([pd.read_csv(path+tag1+'_202009.csv'),\n",
        "                       pd.read_csv(path+tag1+'_202010.csv'),\n",
        "                       pd.read_csv(path+tag1+'_202011.csv'),\n",
        "                       pd.read_csv(path+tag1+'_202012.csv'),\n",
        "                       pd.read_csv(path+tag1+'_202101.csv'),\n",
        "                       pd.read_csv(path+tag2+'_202101.csv'),\n",
        "                       pd.read_csv(path+tag2+'_202102.csv')])\n",
        "\n",
        "tag2_pd = pd.concat([pd.read_csv(path+tag2+'_202009.csv'),\n",
        "                       pd.read_csv(path+tag2+'_202010.csv'),\n",
        "                       pd.read_csv(path+tag2+'_202011.csv'),\n",
        "                       pd.read_csv(path+tag2+'_202012.csv'),\n",
        "                       pd.read_csv(path+tag2+'_202101.csv'),\n",
        "                       pd.read_csv(path+tag2+'_202102.csv')])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Niyoxer_jKLc"
      },
      "source": [
        "tag1_pd['datetime'] = tag1_pd['timestamp'].astype('int').astype(\"datetime64[s]\")\n",
        "tag2_pd['datetime'] = tag2_pd['timestamp'].astype('int').astype(\"datetime64[s]\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiutgA2ijcxo"
      },
      "source": [
        "tag1_normal = tag1_pd[(tag1_pd['datetime'] > datetime.datetime(2020,9,1,0,0)) & (tag1_pd['datetime'] < datetime.datetime(2021,1,8,0,0))]\n",
        "tag1_abnormal = tag1_pd[(tag1_pd['datetime'] > datetime.datetime(2021,1,9,0,0)) & (tag1_pd['datetime'] < datetime.datetime(2021,2,1,0,0))]\n",
        "\n",
        "tag2_normal = tag2_pd[(tag2_pd['datetime'] > datetime.datetime(2020,9,1,0,0)) & (tag2_pd['datetime'] < datetime.datetime(2021,1,8,0,0))]\n",
        "tag2_abnormal = tag2_pd[(tag2_pd['datetime'] > datetime.datetime(2021,1,9,0,0)) & (tag2_pd['datetime'] < datetime.datetime(2021,2,1,0,0))]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNMZ8OLnjc5O"
      },
      "source": [
        "tag1_normal_dt = tag1_normal['datetime']\n",
        "tag1_abnormal_dt = tag1_abnormal['datetime']\n",
        "\n",
        "tag2_normal_dt = tag2_normal['datetime']\n",
        "tag2_abnormal_dt = tag2_abnormal['datetime']\n",
        "\n",
        "original_list = [s for s in list(tag1_normal) if (s.startswith('CREST') or \n",
        "                                                      s.startswith('KURT_') or \n",
        "                                                      s.startswith('SKEW') or \n",
        "                                                      s.startswith('RMS_'))]\n",
        "\n",
        "tag1_normal = tag1_normal[original_list]\n",
        "tag1_abnormal = tag1_abnormal[original_list]\n",
        "\n",
        "tag2_normal = tag2_normal[original_list]\n",
        "tag2_abnormal = tag2_abnormal[original_list]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqWq4C1rkfW-"
      },
      "source": [
        "Xs = tag1_normal.append(tag1_abnormal)\n",
        "Xt = tag2_normal.append(tag2_abnormal)\n",
        "\n",
        "ys = pd.DataFrame(index=range(0,len(tag1_normal)),columns=['label'], dtype='int', data=0).append(pd.DataFrame(index=range(0,len(tag1_abnormal)),columns=['label'], dtype='int', data=1))\n",
        "yt = pd.DataFrame(index=range(0,len(tag2_normal)),columns=['label'], dtype='int', data=0).append(pd.DataFrame(index=range(0,len(tag2_abnormal)),columns=['label'], dtype='int', data=1))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxMh3ScBjKN3"
      },
      "source": [
        "import random\n",
        "\n",
        "shape_min = min (Xs.shape[0], yt.shape[0])\n",
        "shape_max = max (Xs.shape[0], yt.shape[0])\n",
        "\n",
        "index = sorted(random.sample(range(0, shape_max), shape_min))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pd9xMZIIkfZa"
      },
      "source": [
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "min_max_scaler = min_max_scaler.fit(Xs)\n",
        "\n",
        "Xs = pd.DataFrame(min_max_scaler.transform(Xs))\n",
        "Xt = pd.DataFrame(min_max_scaler.transform(Xt))\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkX3JlqTkfjL",
        "outputId": "8569742d-56ee-4745-8c04-45783b7ea735"
      },
      "source": [
        "print(len(Xt), len(Xs))\n",
        "\n",
        "if len(Xt) > len(Xs):\n",
        "    Xt = Xt.iloc[index]\n",
        "    yt = yt.iloc[index]\n",
        "else:\n",
        "    Xs = Xs.iloc[index]\n",
        "    ys = ys.iloc[index]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "51396 75576\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69hClFNjfU2_"
      },
      "source": [
        "def build_models(n_neurons):\n",
        "    \"\"\"Creates three different models, one used for source only training, two used for domain adaptation\"\"\"\n",
        "    inputs = Input(shape=(104,)) \n",
        "    x4 = Dense(n_neurons, activation='linear')(inputs)\n",
        "    x4 = BatchNormalization()(x4)\n",
        "    x4 = Activation(\"elu\")(x4)  \n",
        "\n",
        "    source_classifier = Dense(2, activation='softmax', name=\"mo\")(x4)  \n",
        "    domain_classifier = Dense(32, activation='linear', name=\"do4\")(x4)\n",
        "    domain_classifier = BatchNormalization(name=\"do5\")(domain_classifier)\n",
        "    domain_classifier = Activation(\"elu\", name=\"do6\")(domain_classifier)\n",
        "    domain_classifier = Dropout(0.5)(domain_classifier)\n",
        "\n",
        "    domain_classifier = Dense(2, activation='softmax', name=\"do\")(domain_classifier)\n",
        "\n",
        "    comb_model = Model(inputs=inputs, outputs=[source_classifier, domain_classifier])\n",
        "    comb_model.compile(optimizer=\"Adam\",\n",
        "              loss={'mo': 'categorical_crossentropy', 'do': 'categorical_crossentropy'},\n",
        "              loss_weights={'mo': 1, 'do': 2}, metrics=['accuracy'], )\n",
        "\n",
        "    source_classification_model = Model(inputs=inputs, outputs=[source_classifier])\n",
        "    source_classification_model.compile(optimizer=\"Adam\",\n",
        "              loss={'mo': 'categorical_crossentropy'}, metrics=['accuracy'], )\n",
        "\n",
        "\n",
        "    domain_classification_model = Model(inputs=inputs, outputs=[domain_classifier])\n",
        "    domain_classification_model.compile(optimizer=\"Adam\",\n",
        "                  loss={'do': 'categorical_crossentropy'}, metrics=['accuracy'])\n",
        "    \n",
        "    \n",
        "    embeddings_model = Model(inputs=inputs, outputs=[x4])\n",
        "    embeddings_model.compile(optimizer=\"Adam\",loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
        "                        \n",
        "                        \n",
        "    return comb_model, source_classification_model, domain_classification_model, embeddings_model\n",
        "\n",
        "def batch_generator(data, batch_size):\n",
        "    \"\"\"Generate batches of data.\n",
        "\n",
        "    Given a list of numpy data, it iterates over the list and returns batches of the same size\n",
        "    This\n",
        "    \"\"\"\n",
        "    all_examples_indices = len(data[0])\n",
        "    while True:\n",
        "        mini_batch_indices = np.random.choice(all_examples_indices, size=batch_size, replace=False)\n",
        "        tbr = [k[mini_batch_indices] for k in data]\n",
        "        yield tbr"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmASEoacfU5X"
      },
      "source": [
        "#SAMPLING_ITERATIONS = 3000\n",
        "\n",
        "def train(Xs, ys, Xt, yt,  enable_dann = True, n_iterations = 15000):\n",
        "    \n",
        "    batch_size = 256\n",
        "    \n",
        "    model, source_classification_model, domain_classification_model, embeddings_model = build_models(2)\n",
        "\n",
        "    y_class_dummy = np.ones((len(Xt), 2))\n",
        "    y_adversarial_1 = to_categorical(np.array(([1] * batch_size + [0] * batch_size)))\n",
        "    \n",
        "    sample_weights_class = np.array(([1] * batch_size + [0] * batch_size))\n",
        "    sample_weights_adversarial = np.ones((batch_size * 2,))\n",
        "\n",
        "    S_batches = batch_generator([Xs, to_categorical(ys)], batch_size)\n",
        "    T_batches = batch_generator([Xt, np.zeros(shape = (len(Xt),2))], batch_size)\n",
        "    \n",
        "    for i in range(n_iterations):\n",
        "        # # print(y_class_dummy.shape, ys.shape)\n",
        "        y_adversarial_2 = to_categorical(np.array(([0] * batch_size + [1] * batch_size)))\n",
        "\n",
        "        X0, y0 = next(S_batches)\n",
        "        X1, y1 = next(T_batches)\n",
        "\n",
        "\n",
        "        X_adv = np.concatenate([X0, X1])\n",
        "        y_class = np.concatenate([y0, np.zeros_like(y0)])\n",
        "\n",
        "        adv_weights = []\n",
        "        for layer in model.layers:\n",
        "            if (layer.name.startswith(\"do\")):\n",
        "                adv_weights.append(layer.get_weights())\n",
        "\n",
        "        if(enable_dann):\n",
        "            # note - even though we save and append weights, the batchnorms moving means and variances\n",
        "            # are not saved throught this mechanism \n",
        "            stats = model.train_on_batch(X_adv, [y_class, y_adversarial_1],\n",
        "                                     sample_weight=[sample_weights_class, sample_weights_adversarial])\n",
        "            \n",
        "            k = 0\n",
        "            for layer in model.layers:\n",
        "                if (layer.name.startswith(\"do\")):\n",
        "                    layer.set_weights(adv_weights[k])\n",
        "                    k += 1\n",
        "\n",
        "            class_weights = []\n",
        "            \n",
        "        \n",
        "            for layer in model.layers:\n",
        "                if (not layer.name.startswith(\"do\")):\n",
        "                    class_weights.append(layer.get_weights())\n",
        "\n",
        "            #print(type(X_adv), type(y_adversarial_2))\n",
        "            \n",
        "            stats2 = domain_classification_model.train_on_batch(X_adv, y_adversarial_2)\n",
        "\n",
        "            k = 0\n",
        "            for layer in model.layers:\n",
        "                if (not layer.name.startswith(\"do\")):\n",
        "                    layer.set_weights(class_weights[k])\n",
        "                    k += 1\n",
        "\n",
        "        else:\n",
        "            source_classification_model.train_on_batch(X0,y0)\n",
        "            \n",
        "       \n",
        "        if ((i + 1) % 1000 == 0):\n",
        "            # print(i, stats)\n",
        "            y_test_hat_t = source_classification_model.predict(Xt).argmax(1)\n",
        "            y_test_hat_s = source_classification_model.predict(Xs).argmax(1)\n",
        "            print(\"Iteration %d, source accuracy =  %.3f, target accuracy = %.3f\"%(i, accuracy_score(ys, y_test_hat_s), accuracy_score(yt, y_test_hat_t)))\n",
        "    return embeddings_model"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5smaSBIYos6m",
        "outputId": "9a64f349-f31d-4a93-cde4-2ba4124b5f03"
      },
      "source": [
        "embs = train(Xs.values, ys.values, Xt.values, yt.values, enable_dann = False)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 999, source accuracy =  0.844, target accuracy = 0.595\n",
            "Iteration 1999, source accuracy =  0.826, target accuracy = 0.528\n",
            "Iteration 2999, source accuracy =  0.904, target accuracy = 0.239\n",
            "Iteration 3999, source accuracy =  0.869, target accuracy = 0.526\n",
            "Iteration 4999, source accuracy =  0.371, target accuracy = 0.206\n",
            "Iteration 5999, source accuracy =  0.541, target accuracy = 0.424\n",
            "Iteration 6999, source accuracy =  0.959, target accuracy = 0.620\n",
            "Iteration 7999, source accuracy =  0.856, target accuracy = 0.648\n",
            "Iteration 8999, source accuracy =  0.817, target accuracy = 0.622\n",
            "Iteration 9999, source accuracy =  0.921, target accuracy = 0.516\n",
            "Iteration 10999, source accuracy =  0.915, target accuracy = 0.470\n",
            "Iteration 11999, source accuracy =  0.938, target accuracy = 0.428\n",
            "Iteration 12999, source accuracy =  0.849, target accuracy = 0.693\n",
            "Iteration 13999, source accuracy =  0.874, target accuracy = 0.676\n",
            "Iteration 14999, source accuracy =  0.822, target accuracy = 0.840\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4ufXbP0fU7y",
        "outputId": "17b968f4-e4a9-440c-8445-c06152f6f1f6"
      },
      "source": [
        "\n",
        "embs = train(Xs.values, ys.values, Xt.values, yt.values, enable_dann = True)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 999, source accuracy =  0.459, target accuracy = 0.328\n",
            "Iteration 1999, source accuracy =  0.853, target accuracy = 0.830\n",
            "Iteration 2999, source accuracy =  0.853, target accuracy = 0.837\n",
            "Iteration 3999, source accuracy =  0.852, target accuracy = 0.854\n",
            "Iteration 4999, source accuracy =  0.853, target accuracy = 0.849\n",
            "Iteration 5999, source accuracy =  0.851, target accuracy = 0.848\n",
            "Iteration 6999, source accuracy =  0.723, target accuracy = 0.791\n",
            "Iteration 7999, source accuracy =  0.850, target accuracy = 0.854\n",
            "Iteration 8999, source accuracy =  0.817, target accuracy = 0.855\n",
            "Iteration 9999, source accuracy =  0.879, target accuracy = 0.797\n",
            "Iteration 10999, source accuracy =  0.865, target accuracy = 0.838\n",
            "Iteration 11999, source accuracy =  0.898, target accuracy = 0.824\n",
            "Iteration 12999, source accuracy =  0.882, target accuracy = 0.815\n",
            "Iteration 13999, source accuracy =  0.789, target accuracy = 0.675\n",
            "Iteration 14999, source accuracy =  0.884, target accuracy = 0.848\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQtH4WXdq_cK"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4tZgH-aq_ew"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BmAgVGDq_hI"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-k1i3kq_q_jr"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjxYvv-mtvfr"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}